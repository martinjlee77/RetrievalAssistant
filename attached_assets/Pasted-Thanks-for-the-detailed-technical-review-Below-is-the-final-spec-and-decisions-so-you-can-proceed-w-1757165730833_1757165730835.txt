Thanks for the detailed technical review. Below is the final spec and decisions so you can proceed without ambiguity. I’ve kept it simple and aligned with our early-stage constraints.

Decisions locked (please implement)

Trial and access
Flow: Email signup → admin manual approval → 3 free runs. No magic link in Phase 1. We’ll add magic links later.
Trial limits: 25 MB per run, max 1 concurrent run, 3 free runs total. No credits required during trial.
Pricing and billing
1 credit = $1 (public and internal).
Estimator shows a range (low–high). User pays actual usage, capped at 115% of the displayed high end.
Gating: To start a paid run, require credits_balance ≥ cap (cap = ceil(high × 1.15)).
Failures: 0 credits charged.
Credits expiration: Store expires_at, but do not enforce expiration in Phase 1 (we’ll enforce once Stripe is live).
File size and performance (Phase 1)
Max upload size: 25 MB for trials, 50 MB for approved users. We will revisit after live data.
No persistent storage of files; process in-session only. Only store derived counts/metrics.
Async processing
Use simple threading for background jobs.
Statuses: draft → estimated → running → completed/failed.
UX: After “Analyze,” show “Analysis running…” and a refresh button. No real-time streaming needed for MVP.
Parser and word count
Parsers: pdfplumber (PDF), python-docx (DOCX), PyPDF2 as last resort.
OCR: Off by default for MVP (OCR inflates counts and adds complexity). If no text is extracted, use fallback estimate buckets and allow run.
Word count: deterministic whitespace normalization, then split on whitespace.
Estimator (words-based)
words = count of all parsed text across uploaded files in the session.
base = max(2, floor(words / 2000))
asc_multiplier:
ASC 805: 4.0
ASC 606: 3.0
ASC 842: 2.3
ASC 718: 1.8
ASC 340-40: 1.5
est_mid = base × asc_multiplier
est_low = floor(est_mid × 0.8)
est_high = ceil(est_mid × 1.2)
cap = ceil(est_high × 1.15)
Display: “Estimated cost: est_low–est_high credits. Final charge capped at cap.”
Charge on completion: billed_credits = min(actual_credits, cap); deduct once.
Fallback buckets if parsing fails (or words == 0):
Small (≤2k words): 3–8 credits
Medium (2k–10k): 8–18 credits
Large (10k–25k): 18–35 credits
XL (>25k): 35–50 credits
cap = ceil(high × 1.15)
Concurrency and safety
Limit 1 running analysis per user. If a run is in progress, block new runs until it finishes or fails.
If the process restarts mid-run, mark any “running” older than 45 minutes as failed with error_message = “interrupted”; no charge.
Admin operations (manual-first)
Approve users; set free_analyses_remaining = 3.
Adjust credits manually (top-up) with optional expires_at stored.
View basic ledger and analyses list. For Week 1, a simple CLI or minimal Streamlit admin page is fine.
Minimal data model (3 tables, includes cap fields)

users
id, email, status [pending, approved], credits_balance (decimal 2dp), free_analyses_remaining (int), created_at, approved_at
analyses
id, user_id, asc_standard, status [draft, estimated, running, completed, failed], words_count, estimate_low_credits, estimate_high_credits, estimate_cap_credits, estimate_displayed_at, completed_at, actual_credits, billed_credits, error_message
credit_transactions
id, user_id, analysis_id, amount (neg for charges, pos for grants/top-ups), reason [trial_grant, admin_topup, analysis_charge, expiration], created_at, expires_at (nullable)
API and flow

POST /signup
Input: email. Create user with status=pending. No email verification in Phase 1 (manual approval only).
Admin approve
Set status=approved; set free_analyses_remaining=3.
POST /estimate
Input: asc_standard, session file handles.
Parse files, count words, compute low/high/cap, persist to analyses (status=estimated), return range and cap.
POST /runs
Validate:
user.status == approved
If free_analyses_remaining > 0 → allow.
Else require credits_balance ≥ estimate_cap_credits and no other running analysis for this user.
Set status=running; start background thread.
PATCH /runs/:id/complete
Input: actual token/usage → compute actual_credits (we’ll aggregate from SDK or approximate until we wire exact tokens).
If trial: billed_credits=0; decrement free_analyses_remaining by 1.
Else: billed_credits = min(actual_credits, estimate_cap_credits); deduct and write credit_transactions row atomically.
Set completed_at and status=completed; persist actual_credits and billed_credits.
Failure path
On error: status=failed, billed_credits=0.
UI strings

Paid with balance:
“Estimated cost: X–Y credits. Final charge capped at Z. You have B credits.”
Trial:
“This run will be free (trial). Estimated cost: X–Y credits.”
Insufficient credits:
“Estimated cost: X–Y credits (cap Z). You have B. Contact admin to add credits.”
Testing and acceptance

Start run with sufficient balance; completion charges min(actual, cap).
Trial run bypasses balance; decrements free counter on completion.
Parsing failure invokes fallback buckets and still applies cap.
One analysis at a time per user enforced.
Restart safety: old “running” analyses (>45 min) marked failed; no charge.
Ledger entries created exactly once; balances accurate to 2 decimals.
Security/limits

Allowed file types: PDF, DOCX only for Phase 1 (block others).
Size limits enforced at upload (25 MB trial, 50 MB approved).
No raw text persisted; only counts and derived metrics.
Basic input validation and MIME/type sniffing to avoid malformed uploads.
Open items for you to confirm or pick

Background job: Any preference between threading.Thread vs ThreadPoolExecutor? Either is fine; pick what’s simplest on our stack.
Parser libraries: Are pdfplumber, python-docx, and PyPDF2 all available and stable in our environment?
Upload limits: Comfortable with 25 MB (trial) and 50 MB (approved) as hard limits for Phase 1?
Admin: Start with CLI or minimal Streamlit page (approve users, grant credits, view ledger)? Please choose.
Actual usage measurement: For MVP, we can record a simple actual_credits value from the model SDK’s token counts if available; otherwise, use est_mid until we wire real counts. Which do you prefer to ship fastest?
Once you confirm these, please provide an effort estimate and any immediate dependencies you need (e.g., environment packages, email provider if we later add magic links, etc.). I’ll finalize the on-screen copy and ASC multiplier tuning with you right before we push to production.