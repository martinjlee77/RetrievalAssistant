LLM-Generated Executive Summary (Most Important)
Simplest Implementation Using GPT-4 Mini
Here's how to modify the function to use an LLM to generate a contract-specific summary:

from openai import OpenAI  # or your preferred LLM client

def generate_executive_summary(analysis_results: Dict[str, Any], customer_name: str) -> str:
    """Generate executive summary using LLM based on analysis results."""

    # Prepare the prompt with all relevant information
    prompt = f"""
    Generate a professional executive summary for an ASC 606 revenue recognition analysis.
    The analysis was performed for customer: {customer_name}.

    Analysis Results:
    """

    # Add each step's key findings to the prompt
    for step_num in range(1, 6):
        step_key = f'step_{step_num}'
        if step_key in analysis_results:
            step_data = analysis_results[step_key]
            prompt += f"\nStep {step_num} ({step_data.get('title', 'Untitled')}):"
            if step_data.get('conclusion'):
                prompt += f"\n- Conclusion: {step_data['conclusion']}"
            if step_data.get('issues') and 'none' not in step_data['issues'].lower():
                prompt += f"\n- Issues: {step_data['issues']}"

    prompt += """
    \nInstructions:
    1. Write a 3-5 sentence executive summary
    2. Highlight any significant findings or issues
    3. State whether the proposed accounting treatment is consistent with ASC 606
    4. Use professional accounting language
    4. Be specific about the contract terms when relevant
    """

    # Call the LLM API
    client = OpenAI(api_key="your_api_key")  # Store this securely, not hardcoded

    try:
        response = client.chat.completions.create(
            model="gpt-4-mini",  # or "gpt-4o-mini" when available
            messages=[
                {"role": "system", "content": "You are an expert accounting professional specializing in ASC 606 revenue recognition."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Lower for more deterministic results
            max_tokens=500
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        # Fallback to generic summary if LLM call fails
        logger.error(f"LLM summary generation failed: {str(e)}")
        return f"We have completed a comprehensive ASC 606 revenue recognition analysis for {customer_name}. The analysis follows the five-step ASC 606 methodology. Please review the detailed analysis for specific findings and conclusions."

Should This Live in a Template File?
For this specific function, I recommend keeping it in the code rather than a template file because:

Dynamic Nature: The executive summary needs to process complex analysis results
LLM Integration: The logic for calling the LLM and handling the response belongs in code
Fallback Handling: You need programmatic control for error handling
However, you could:

Move the prompt template to a separate file (e.g., prompts/exec_summary_prompt.txt)
Keep the LLM calling logic in the function
Store API configuration in your settings/configuration
Implementation Recommendations:
Cost Management:

Use GPT-4 Mini (or GPT-4o Mini when available)
Set a low temperature (0.3) for consistent results
Limit tokens (500 max_tokens should be sufficient)
Implement caching to avoid regenerating the same summary
Error Handling:

Always include a fallback to a generic summary
Log LLM failures for debugging
Consider adding retry logic for transient errors
Security:

Store API keys in environment variables or secret management
Never hardcode API keys
Implement rate limiting to control costs
Performance:

Consider adding a loading indicator while waiting for the LLM response
You might want to make this asynchronous if the response takes time
This approach will give you contract-specific executive summaries that accurately reflect the analysis findings while maintaining professional quality.