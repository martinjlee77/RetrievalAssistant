Goal

Add a simple, transparent cost estimate before the user runs “Analyze & Generate,” priced in credits, with a hard cap so the final charge cannot exceed the displayed high end. Keep UX minimal and predictable.
What we’re shipping (scope)

After upload and ASC selection, show an estimated credit range and time above the Analyze & Generate button.
Users have a monthly plan (50 credits for $50). Additional usage is metered in credits.
We only charge when the run completes. Final billing is capped at the displayed high end.
No mid-run confirmation or cost-cap controls. If balance is insufficient, the user is asked to add credits.
A tiny “View details” breakdown is optional and collapsible.
We log actual vs estimated usage per run for later tuning.
Non-goals (for now)

No bring-your-own-key.
No exclusions of document sections.
No advanced depth controls or enterprise packaging beyond the basic plan.
UX requirements

Placement: Show an estimate line directly above or to the left of the Analyze & Generate button.
Default copy:
“Estimated cost: X–Y credits • Est. A–B min • You have Z credits.”
Tooltip on an info icon: “Estimate based on document length and selected standard. Final charge will not exceed the high end.”
If insufficient credits: replace the button with “Add credits to proceed” or disable it and show an inline link.
“View details” (collapsed by default) shows:
Document length: ~N tokens
Standard: ASC ### (e.g., 606)
Estimated tokens: Low–High
Estimated cost: Low–High credits
States:
Sufficient credits: button enabled.
Insufficient credits: button disabled; show “Estimated cost: 22–28 credits. You have 14. Add credits to proceed.”
Error/fallback state: if estimator can’t compute, show size bucket guidance (Small/Medium/Large/XL) with a range and keep the cap policy.
Pricing model

Plan: 
50
/
m
o
n
t
h
i
n
c
l
u
d
e
s
50
c
r
e
d
i
t
s
.
A
d
d
i
t
i
o
n
a
l
c
r
e
d
i
t
s
b
i
l
l
e
d
a
t
50/monthincludes50credits.Additionalcreditsbilledat1/credit.
All internal costs are converted to credits. We will not expose vendor pricing; we sell the outcome.
Billing cap: charge min(actual_credits, estimate_high_credits).
Estimator algorithm (initial simple version)

Configurable parameters (env or config file):
fixed_overhead_tokens = 20000
asc_factor (tokens per document token):
606: 2.5
842: 2.2
718: 2.0
805: 3.0
340-40: 2.3
buffer_percent_for_display = 0.20
credits_per_million_tokens = [set in config; derived from our blended internal model cost and desired margin]
tokens_per_second_for_time_estimate = [e.g., 400] or tokens_per_minute = [e.g., 24000]
Approximate document tokens:
Use chars_to_tokens ≈ max(4, total_chars / 4) or words_to_tokens ≈ words * 1.3. Pick one; chars/4 is simplest.
total_chars includes all uploaded files after parsing/OCR.
Compute:
approx_doc_tokens = chars / 4
asc_multiplier = asc_factor[selected_ASC]
est_tokens_mid = fixed_overhead_tokens + approx_doc_tokens * asc_multiplier
est_tokens_low = est_tokens_mid * (1 - buffer_percent_for_display)
est_tokens_high = est_tokens_mid * (1 + buffer_percent_for_display)
credits_low = tokens_to_credits(est_tokens_low)
credits_high = tokens_to_credits(est_tokens_high)
time_low/high = est_tokens_low/high / tokens_per_minute
Display round rules:
Display credits rounded to whole numbers (e.g., 8–12). Internally keep 2 decimal places in the ledger.
Display time rounded to minutes (e.g., 3–6 min).
Fallback if estimator can’t compute (e.g., parsing failed)

Use size buckets by approx file size if tokenization fails:
Small (≤10k tokens): 3–6 credits
Medium (10–50k): 6–15 credits
Large (50–150k): 15–40 credits
XL (>150k): 40–80 credits
Still apply the hard cap at the high end displayed.
Run gating and charging

To keep things simple and avoid mid-run top-ups, require available credits ≥ estimated high end to start a run.
When the run completes:
Compute actual_tokens_used across all steps (input + output tokens for all LLM calls, plus retrieval overhead if charged).
actual_credits = tokens_to_credits(actual_tokens_used)
bill_credits = min(actual_credits, displayed_estimate_high_credits)
Deduct bill_credits from user balance; write a detailed cost log to the run record.
If the run fails due to system errors, do not charge; optionally deduct a small fixed diagnostic credit if we ever add an explicit “preview-only” mode (not in scope now).
Instrumentation to measure actuals

For each LLM call, capture:
model_name, timestamp, step_name, input_tokens, output_tokens, total_tokens, retries
For retrieval steps, capture:
passages_retrieved, tokens_in_passages (approx), chunking params
Persist per-run aggregates:
total_tokens_input, total_tokens_output, total_tokens, steps_count, retries_count
Persist estimate shown to the user: est_low_tokens, est_high_tokens, est_low_credits, est_high_credits, asc, approx_doc_tokens.
Data model additions

User: id, credits_balance (decimal, 2 dp), plan_type
CreditLedger: id, user_id, delta, reason, run_id, created_at
RunJob: id, user_id, asc, status, created_at, completed_at, estimate_low_credits, estimate_high_credits, estimate_low_tokens, estimate_high_tokens, approx_doc_tokens, displayed_to_user_at
RunCostActuals: run_id, total_input_tokens, total_output_tokens, total_tokens, actual_credits, billed_credits
Config table or static config (with asc_factor, fixed_overhead_tokens, credits_per_million_tokens, buffer_percent, tokens_per_minute)
API and UI wiring

POST /estimate
Input: asc, uploaded_file_ids
Steps: parse files, count chars, compute approx_doc_tokens, compute estimate, persist estimate fields on a draft RunJob
Output: credits_low, credits_high, time_low_min, time_high_min, approx_doc_tokens, optional breakdown text
POST /runs
Validates that credits_balance ≥ estimate_high_credits for the draft run
Transitions RunJob to “running”
PATCH /runs/:id/complete
Input: actual token metrics by step
Computes bill_credits and deducts from ledger
Marks RunJob complete with actuals
GET /runs/:id/cost
Returns estimate, actuals, and a printable breakdown for audit trail (optional now; foundation for later)
Core functions (pseudo)

approx_doc_tokens(chars):
return max(4, floor(chars / 4))
estimate_tokens(asc, approx_doc_tokens):
return fixed_overhead_tokens + asc_factor[asc] * approx_doc_tokens
tokens_to_credits(tokens):
return round_to_2dp(tokens / 1_000_000 * credits_per_million_tokens)
estimate_to_range(est_tokens_mid):
low = est_tokens_mid * (1 - buffer_percent)
high = est_tokens_mid * (1 + buffer_percent)
return low, high
Error handling and edge cases

Parsing failure or zero text: show a friendly error and block the run. Offer re-upload.
Extremely large uploads: enforce a max total chars/tokens and show a message to contact support.
Network/model retries: include retries’ tokens in actuals.
Concurrency: ensure ledger updates are transactional to avoid race conditions on balance.
Rounding: always store credits with 2 decimals; display whole numbers.
Testing plan

Unit tests
chars→tokens conversion
asc_factor mapping and estimator math
tokens→credits conversion and rounding
billing cap min(actual, high)
Integration tests
Normal flow with sufficient balance
Insufficient balance blocks run
Fallback bucket used when parsing fails
Actual tokens > high end still bills at high end
Ledger accuracy and idempotency on retries
Sample docs
Short, medium, long for each ASC to validate variance
Accuracy target (initial)
Median actual within 0.8–1.2x of midpoint estimate across 20–50 runs; we’ll tune asc_factors and fixed_overhead as we gather data.
Analytics and tuning

Log for each run: asc, approx_doc_tokens, estimate_mid_tokens, actual_tokens, actual/estimate ratio.
Build a simple dashboard or query to see ratio distributions and adjust asc_factors monthly.
Track abandonment when insufficient credits are shown.
Config and feature flags

All numeric parameters in config or env so we can adjust without redeploy:
fixed_overhead_tokens, asc_factor map, buffer_percent, credits_per_million_tokens, tokens_per_minute
Feature flag: ENABLE_COST_ESTIMATOR (allows quick rollback).
Security and privacy

Do not log raw document text in analytics. Store only counts and derived metrics.
Ensure uploads are encrypted at rest and in transit. Mask PII in logs if present.
Rollout

Release behind a feature flag.
Soft launch to internal test users with a few real contracts.
Validate that billing cap works and ledger balances reflect deductions exactly once per run.
Copy strings

“Estimated cost: X–Y credits • Est. A–B min • You have Z credits.”
Tooltip: “Estimate based on document length and selected standard. Final charge will not exceed the high end.”
Insufficient balance: “Estimated cost: X–Y credits. You have Z. Add credits to proceed.”
Fallback bucket: “We could not precisely estimate from the upload. Based on size, expect N–M credits. Final charge will not exceed M.”
Open questions for you

Confirm we should require balance ≥ estimated high end to start a run. This avoids mid-run funding and keeps logic simple.
Confirm whether we allow fractional credits in the ledger (recommended: yes, store to 2 decimals; display whole numbers).
Provide initial value for credits_per_million_tokens (we can start with a conservative buffer).
Set initial tokens_per_minute for the time estimate. We can adjust after a week of data.
Any upper limit for total upload size per run we should enforce on day one?
If this looks good, please estimate effort and list any dependencies you need (e.g., token counts from the model SDK, parsing library choices, where to persist config). Happy to clarify any part.

Thanks!